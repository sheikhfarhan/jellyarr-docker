# ðŸ”§ Troubleshooting & Diagnostics Playbook

This document records the specific commands and logic used to diagnose complex issues in this stack. Use this guide when things break to identify *why* before trying to fix it.

-----

## 1\. High CPU / Resource Usage

### **Symptom**

System fans spin up, load average spikes, or UI becomes unresponsive.

### **Investigation Command**

Use `docker stats` to instantly identify the offending container. The `--no-stream` flag gives a clean snapshot instead of a jumping live feed.

```bash
docker stats --no-stream
```

### **Case Study: CrowdSec at 400% CPU**

  * **Observation:** `crowdsec` container was using 390%+ CPU.
  * **Diagnosis:** CrowdSec was parsing *every single log line* generated by Caddy.
  * **Root Cause:** Homepage widgets check service status every few seconds. Caddy was logging these internal requests (`172.20.x.x`), flooding CrowdSec with "clean" traffic to analyze.
  * **The Fix:** configured Caddy to **skip logging** for internal IPs.
      * *Code:* `log_skip @internal` in `Caddyfile`.

-----

## 2\. Connection Refused / Service Down

### **Symptom**

A service (like Portainer or WUD) cannot connect to another service (like Socket Proxy), showing `ECONNREFUSED` or `ENOTFOUND`.

### **Investigation Command**

Don't just guessâ€”check if the target is actually listening on the expected port from *inside* the network.

```bash
# 1. Check if the target is running
docker ps | grep socket-proxy

# 2. Check container logs for startup errors
docker logs socket-proxy

# 3. Verify Internal DNS resolution (from another container)
docker exec -it wud ping socket-proxy
```

### **Case Study: WUD & Portainer vs. Socket Proxy**

  * **Observation:** WUD logs showed `getaddrinfo ENOTFOUND socket-proxy`.
  * **Diagnosis:** Docker's internal DNS sometimes fails to resolve container names immediately on boot or across stack definitions.
  * **The Fix:** Switch from **Hostname** (`socket-proxy`) to **Static IP** (`172.20.0.29`).
      * *Config:* `WUD_WATCHER_PROXY_HOST=172.20.0.29`

-----

## 3\. Storage / Disk Missing in Dashboard

### **Symptom**

Homepage reports `Drive not found for target: /mnt/dockerapps_disk`.

### **Investigation Command**

Verify what the container *actually* sees mounted.

```bash
docker exec homepage df -h
```

### **Case Study: The "Ghost" Mount**

  * **Observation:** `df -h` inside the container did NOT show the mount, even though `compose.yml` had it.
  * **Diagnosis:** **Startup Race Condition.** Docker started *before* the OS finished mounting the LVM drive. Docker bound the empty folder on the root drive instead of the actual disk.
  * **The Fix:**
    1.  **Simplified:** Removed the extra mount entirely.
    2.  **Redirected:** Pointed the widget to `/app/config` (which is naturally inside the container) instead of an external `/mnt` path. Since the container *lives* on that disk, if the container runs, the disk is there.

-----

## 4\. External Access Fails (Mobile Only)

### **Symptom**

Jellyfin works on LAN (Wi-Fi) and Desktop Browser (4G), but **fails on Android App (4G)**.

### **Investigation Tool**

**SSL Labs Server Test** (ssllabs.com) and Cloudflare DNS Dashboard.

### **Case Study: The IPv6 Trap**

  * **Observation:** Browser loaded the site, but App said "Connection Cannot be Established."
  * **Investigation:** SSL Labs showed the IPv4 test passed ("Ready"), but the IPv6 test failed ("Unable to connect").
  * **Diagnosis:** 4G Mobile networks prioritize **IPv6**. Cloudflare was publishing an AAAA (IPv6) record, but our Docker host wasn't routing IPv6 ingress correctly. The phone tried IPv6, hung, and failed. The browser was smart enough to fallback to IPv4; the app was not.
  * **The Fix:** **Deleted AAAA records** in Cloudflare. Forced the phone to use the robust IPv4 route.

-----

## 5\. Security Verification (Lateral Movement)

### **Symptom**

You need to verify that a compromised container cannot hack your home PC.

### **Investigation Command**

The "Hack Test." Try to ping your router or PC *from inside* a container.

```bash
docker exec -it jellyfin ping 192.168.0.1
```

### **Case Study: The Software VLAN**

  * **Observation:** Originally, the ping succeeded (`time=0.4ms`). This meant zero isolation.
  * **The Fix:** Applied a **Firewalld Direct Rule** to the `DOCKER-USER` chain to DROP packets from `172.20.0.0/24` (Docker) to `192.168.0.0/24` (LAN).
  * **Verification:** Running the command again resulted in **100% Packet Loss**. Success.


## 6\. Homepage 

### **Issue 1: "API Error" on Storage Widgets**

  * **Symptom:** The disk bar shows "API Error" instead of usage stats.
  * **Cause:** The path defined in `widgets.yaml` (e.g., `/mnt/media_disk`) does not exist inside the container.
  * **Fix:** Ensure we have mounted the volume in `compose` (`- /mnt/pool01/media:/mnt/media_disk:ro`)

### **Issue 2: "Host validation failed"**

  * **Symptom:** A white screen with this error message when loading the dashboard.
  * **Cause:** Security feature blocking unknown domains/IPs.
  * **Fix:** Set `HOMEPAGE_ALLOWED_HOSTS=*` in `compose`.

### **Issue 3: Docker Stats Missing / Permission Denied**

  * **Symptom:** Container status dots are grey/missing, or logs show `EACCES /var/run/docker.sock`.
  * **Cause:** The container user (1000) cannot read the host's Docker socket (owned by root/docker).
  * **Fix:** Set `PGID` in `compose` to match the host's docker group ID.
      * Run `getent group docker` on host to find the ID (e.g., 958).

### **Issue 4: CrowdSec Widget Error**

  * **Symptom:** Widget shows error or fails to load.
  * **Cause:** Often caused by using a Bouncer API Key instead of Machine Credentials, or because the `local_api_credentials.yaml` file changed (if the DB was wiped).
  * **Fix:** Verify the username/password in `.env` matches the *current* contents of `local_api_credentials.yaml`

-----

## 7\. General "Rule of Thumb" Workflow

1.  **Check State:** `docker ps -a` (Is it restarting? Exited?)
2.  **Check Logs:** `docker logs <container_name> --tail 50` (Read the actual error)
3.  **Check Access:** `curl -v http://<ip>:<port>` (Is the port actually open?)
4.  **Check Mounts:** `docker inspect -f '{{ .Mounts }}' <container_name>` (Did the volume map correctly?)

